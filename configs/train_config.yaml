
weights:
  source: pretrained
  model: gemma-2b
  cache_dir: null # needs to be set by yq

tokenizer:
  name: google/gemma-2b

dataset:
  type: text
  dataset_name: wikitext
  dataset_config: wikitext-2-raw-v1
  block_size: 1024
  text_column: text
  train_split: train
  eval_split: validation
  bos_token: '<bos>'
  eos_token: '<eos>'

optimizer:
    optim_type: adam
    grad_accum_steps: 8
    lr_schedule:
      sched_type: cosine_with_warmup
      warmup_steps: 10
      lr: 5e-5
    beta1: 0.9
    beta2: 0.95
    weight_decay: 0
    eps: 1e-5